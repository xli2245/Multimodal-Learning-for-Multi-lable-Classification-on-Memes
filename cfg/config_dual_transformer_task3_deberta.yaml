dataset:
  root: 'data'  # not needed if using pre-extracted bottom-up features
  task: 3

text-model:
  name: 'deberta'
  pretrain: 'microsoft/deberta-base'
  word-dim: 768
  extraction-hidden-layer: 10
  fine-tune: False

image-model:
  name: 'resnet50'
  enabled: True
  fine-tune: False
  feat-dim: 2048
  grid: [7, 7]

model:
  name: 'dual-transformer'
  embed-dim: 1024
  feedforward-dim: 1024
  num-encoder-layers: 4
  num-decoder-layers: 4
  model-fusion: 'weighted' # average, weighted, concat, MLP

training:
  balanced-sampling: False
  lr: 0.00005  # 0.000006
  pretrained-modules-lr: 0.000005  # learning rate for early pretrained layers
  grad-clip: 2.0
  bs: 8
  scheduler: 'steplr'
  milestones: [10, 20]
  gamma: 0.1